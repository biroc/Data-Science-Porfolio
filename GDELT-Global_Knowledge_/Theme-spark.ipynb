{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global News Tone Analysis and Visulization\n",
    "------------------------------------------\n",
    "\n",
    "Analyze global news average tone from GDELT Global Knowledge Pubic Dataset in Big Query\n",
    "\n",
    "100,000,000 global news with domain, tone, country mentioned in the news attributes.\n",
    "Tones toward other countries is calculated based on the countries mentioned in the news and the normalized average new tone.\n",
    "\n",
    "The analysis is doen in both Pandas and Spark environment\n",
    "\n",
    "Output Analysis Result\n",
    "------------------------------------------\n",
    "\n",
    "General Average tone in Chinese news toward rest of the world\n",
    "\n",
    "General Average tone in U.S. news toward rest of the world\n",
    "\n",
    "General Average tone in U.K. news toward rest of the world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "1. [Input Data](#Input-Data)\n",
    "2. [Merging Dataset](#Merging-Dataset)\n",
    "3. [Filtering Theme](#Filtering-Theme)\n",
    "4. [Cleaning Data](#Cleaning-Data)\n",
    "5. [Calculating worldwide Average Tones of News from China, US, UK](#Calculating-worldwide-Average-Tones-of-News-from-China,-US,-UK)\n",
    "6. [Filtering Result](#Filtering-Result)\n",
    "7. [Visualizaing Worldwide Average Tone](#Visualizaing-Worldwide-Average-Tone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "import json\n",
    "import re\n",
    "\n",
    "## Filter Function\n",
    "def mask(df, key, value):\n",
    "    return df[df[key] == value]\n",
    "pd.DataFrame.mask = mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = (SparkConf()\n",
    "         .setMaster(\"local\")\n",
    "         .setAppName(\"My app\")\n",
    "         .set(\"spark.executor.memory\", \"16g\"))\n",
    "sc = SparkContext(conf = conf)\n",
    "#sc = pyspark.SparkContext()\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import mean, min, max\n",
    "from pyspark.sql.types import StringType, IntegerType, StructType, StructField, FloatType,ArrayType,DataType\n",
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'1.6.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "###########Input Tone, Location Raw Data#############\n",
    "\n",
    "sqlCtx = SQLContext(sc)\n",
    "df = sqlCtx.read.format('com.databricks.spark.csv').option(\"header\", \"true\").load(\"Data/Theme/spark/*.csv\") \n",
    "\n",
    "sourceCountry = sqlCtx.read.format('com.databricks.spark.csv')\\\n",
    "                .option(\"header\", \"true\").load('Data/sourceCountry.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Dataset\n",
    "\n",
    "Filter News from China, US, UK respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tone = df.join(sourceCountry, df.SourceCommonName == sourceCountry.Domain, 'left')\n",
    "tone = tone.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- SourceCommonName: string (nullable = true)\n",
      " |-- Locations: string (nullable = true)\n",
      " |-- V2Tone: string (nullable = true)\n",
      " |-- V2Themes: string (nullable = true)\n",
      " |-- Domain: string (nullable = true)\n",
      " |-- FIPS: string (nullable = true)\n",
      " |-- CountryName: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tone.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "china = tone[tone['CountryName'] == 'China']\n",
    "us = tone[tone['CountryName'] == 'United States']\n",
    "uk = tone[tone['CountryName'] == 'United Kingdom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "######################################################\n",
    "###############Clean the Tone Data####################\n",
    "#from pyspark.sql.functions import UserDefinedFunction\n",
    "#pattern = re.compile(\"^\\s+|\\s*,\\s*|\\s+$\")\n",
    "\n",
    "#udf = UserDefinedFunction(lambda x: float(pattern.split(x)[0]))\n",
    "\n",
    "pattern = re.compile(\"^\\s+|\\s*,\\s*|\\s+$\")\n",
    "#def cleanTone(Tone):\n",
    "    #return \n",
    "    \n",
    "UDF = udf(lambda x: float(pattern.split(x)[0]),FloatType())\n",
    "\n",
    "china = china.withColumn('Tone',UDF(china.V2Tone))\n",
    "us = us.withColumn('Tone',UDF(us.V2Tone))\n",
    "uk = uk.withColumn('Tone',UDF(uk.V2Tone))\n",
    "\n",
    "#us = us.map(lambda x:float(pattern.split(x.ToneV2)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "#######Clean the Location data mentioned in the news#######\n",
    "\n",
    "def cleanTargetCountry(Loc,Tone):\n",
    "    count={}\n",
    "    result = {}\n",
    "    for i in Loc.split(';'):\n",
    "        key = i.split('#')[2][:2]\n",
    "        if  key not in count.keys():\n",
    "            count[key] = 1\n",
    "        else:\n",
    "            count[key] += 1\n",
    "    totalCount = float(sum(count.values()))\n",
    "    for i in Loc.split(';'):\n",
    "        key = i.split('#')[2][:2]\n",
    "        result[key] = count[key] / totalCount * Tone\n",
    "        \n",
    "    result = json.dumps(result)\n",
    "    return result\n",
    "\n",
    "UDF2 = udf(cleanTargetCountry)\n",
    "us = us.withColumn('Target',UDF2(us.Locations,us.Tone))\n",
    "china = china.withColumn('Target',UDF2(china.Locations,china.Tone))\n",
    "uk = uk.withColumn('Target',UDF2(uk.Locations,uk.Tone))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- SourceCommonName: string (nullable = true)\n",
      " |-- Locations: string (nullable = true)\n",
      " |-- V2Tone: string (nullable = true)\n",
      " |-- V2Themes: string (nullable = true)\n",
      " |-- Domain: string (nullable = true)\n",
      " |-- FIPS: string (nullable = true)\n",
      " |-- CountryName: string (nullable = true)\n",
      " |-- Tone: float (nullable = true)\n",
      " |-- Target: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "us.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Theme \n",
    "\n",
    "Filtering databased on News' Themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "###########Clean the raw theme string######################\n",
    "\n",
    "\n",
    "pattern = re.compile(\"^\\s+|\\s*,\\s*|\\s+$\")\n",
    "\n",
    "def themeFilter(row):\n",
    "    themeList = row.split(';')\n",
    "    #print themeList\n",
    "    for i in range(len(themeList)):\n",
    "        themeList[i] = pattern.split(themeList[i])[0]\n",
    "    return themeList\n",
    "UDF3 = udf(themeFilter,ArrayType(StringType()))\n",
    "us = us.withColumn('Themes',UDF3(us.V2Themes))\n",
    "uk = uk.withColumn('Themes',UDF3(uk.V2Themes))\n",
    "china = china.withColumn('Themes',UDF3(china.V2Themes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "###########Filter data based on the theme##################\n",
    "\n",
    "theme = \"GENERAL_GOVERNMENT\"\n",
    "\n",
    "us_themeFilter = us.where(array_contains(us.Themes, theme))\n",
    "uk_themeFilter = uk.where(array_contains(uk.Themes, theme))\n",
    "china_themeFilter = china.where(array_contains(china.Themes, theme))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- SourceCommonName: string (nullable = true)\n",
      " |-- Locations: string (nullable = true)\n",
      " |-- V2Tone: string (nullable = true)\n",
      " |-- V2Themes: string (nullable = true)\n",
      " |-- Domain: string (nullable = true)\n",
      " |-- FIPS: string (nullable = true)\n",
      " |-- CountryName: string (nullable = true)\n",
      " |-- Tone: float (nullable = true)\n",
      " |-- Target: string (nullable = true)\n",
      " |-- Themes: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "us_themeFilter.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating worldwide Average Tones of News from China, US, UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dawei/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/dawei/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/dawei/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/dawei/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/dawei/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/dawei/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "##################Initial Result Dataframe#################\n",
    "\n",
    "## China\n",
    "countryInfo = pd.read_csv('Data/countryInfo.csv', index_col = False, low_memory = False)\n",
    "\n",
    "china_result = countryInfo[['fips','country']]\n",
    "china_result['tone'] = 0\n",
    "china_result['nb_article'] = 0\n",
    "china_result = china_result.set_index('fips')\n",
    "\n",
    "## U.S.\n",
    "us_result = countryInfo[['fips','country']]\n",
    "us_result['tone'] = 0\n",
    "us_result['nb_article'] = 0\n",
    "us_result = us_result.set_index('fips')\n",
    "\n",
    "## U.K\n",
    "uk_result = countryInfo[['fips','country']]\n",
    "uk_result['tone'] = 0\n",
    "uk_result['nb_article'] = 0\n",
    "uk_result = uk_result.set_index('fips')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 102 in stage 4.0 failed 1 times, most recent failure: Lost task 102.0 in stage 4.0 (TID 343, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 150315 ms\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3456fb06d886>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#############Maintain the tone result to result df#########\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mus_themeFilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcountry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dawei/spark-1.6.2-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \"\"\"\n\u001b[1;32m    770\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dawei/spark-1.6.2-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 813\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dawei/spark-1.6.2-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dawei/spark-1.6.2-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    307\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 102 in stage 4.0 failed 1 times, most recent failure: Lost task 102.0 in stage 4.0 (TID 343, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 150315 ms\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "###########Calculate Number of Articles for each target####\n",
    "#############Maintain the tone result to result df#########\n",
    "\n",
    "for row in us_themeFilter.rdd.collect():\n",
    "    target = json.loads(row['Target'])\n",
    "    for country in target.keys():\n",
    "        try:\n",
    "            us_result.ix[country,'tone'] += target[country]\n",
    "            us_result.ix[country,'nb_article'] +=1\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "\n",
    "for row in uk_themeFilter.rdd.collect():\n",
    "    target = json.loads(row['Target'])\n",
    "    for country in target.keys():\n",
    "        try:\n",
    "            uk_result.ix[country,'tone'] += target[country]\n",
    "            uk_result.ix[country,'nb_article'] +=1\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "for row in china_themeFilter.rdd.collect():\n",
    "    target = json.loads(row['Target'])\n",
    "    for country in target.keys():\n",
    "        try:\n",
    "            china_result.ix[country,'tone'] += target[country]\n",
    "            china_result.ix[country,'nb_article'] +=1\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######Calculate Average tone for other countries for China, US, UK\n",
    "\n",
    "\n",
    "us_result['tone'] = us_result['tone']/us_result['nb_article']\n",
    "china_result['tone'] = china_result['tone']/china_result['nb_article']\n",
    "uk_result['tone'] = uk_result['tone']/uk_result['nb_article']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uk_result.nb_article.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Filtering Result\n",
    "\n",
    "Filtering Top 20 Negative tone country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "##################Filter Data##############################\n",
    "\n",
    "## China\n",
    "countryGeo = pd.read_csv('Data/countrygeo.csv',index_col = False,low_memory = False)\n",
    "countryGeo.columns = ['fips','lat','lon','country']\n",
    "\n",
    "\n",
    "china_result = china_result.dropna()\n",
    "china_result = china_result.sort(columns = 'nb_article',ascending=False)[:200]\n",
    "china_result = pd.merge(china_result,countryGeo, on = ['country'],how = 'left')\n",
    "china_result = china_result.dropna()\n",
    "china_result = china_result[china_result['nb_article']>10]\n",
    "\n",
    "## US\n",
    "us_result = us_result.dropna()\n",
    "us_result = us_result.sort(columns = 'nb_article',ascending=False)[:200]\n",
    "us_result = pd.merge(us_result,countryGeo, on = ['country'],how = 'left')\n",
    "us_result = us_result.dropna()\n",
    "us_result = us_result[us_result['nb_article']>10]\n",
    "\n",
    "## UK\n",
    "uk_result = uk_result.dropna()\n",
    "uk_result = uk_result.sort(columns = 'nb_article',ascending=False)[:200]\n",
    "uk_result = pd.merge(uk_result,countryGeo, on = ['country'],how = 'left')\n",
    "uk_result = uk_result.dropna()\n",
    "uk_result = uk_result[uk_result['nb_article']>10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "china_result_20 = china_result.sort(columns = 'tone',ascending=True)[:20]\n",
    "us_result_20 = us_result.sort(columns = 'tone',ascending=True)[:20]\n",
    "uk_result_20 = uk_result.sort(columns = 'tone',ascending=True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizaing Worldwide Average Tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "china_Map = folium.Map(location=[35.85, 104.19], zoom_start=1)\n",
    "us_Map = folium.Map(location=[35.85, 104.19], zoom_start=1)\n",
    "uk_Map = folium.Map(location=[35.85, 104.19], zoom_start=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "'''\n",
    "for index, row in test2.iterrows():\n",
    "    print index\n",
    "    Location = [row['lat'],row['lon']]\n",
    "    name = row['country']\n",
    "    R = abs(row.tone)\n",
    "    if row.tone>0:\n",
    "        Color = '#3186cc'\n",
    "    else:\n",
    "        Color = '#cc3131'\n",
    "    if name == 'Turkey':\n",
    "        print name\n",
    "    if index>30:\n",
    "        folium.CircleMarker(location=Location, radius=1000000*R,\n",
    "                            popup=name,\n",
    "                            color=Color,\n",
    "                            fill_color=Color).add_to(Map)\n",
    "\n",
    "'''\n",
    "def draw(Map,loc,r,name):\n",
    "    if r > 0.05:\n",
    "        Color = '#3186cc'\n",
    "    elif r < -0.05:\n",
    "        Color = '#cc3131'\n",
    "    else:\n",
    "        Color = '#0ed148'#0ed148\n",
    "    \n",
    "    folium.CircleMarker(location=loc, radius=700000*abs(r),\n",
    "                            popup=name+': '+str(r),\n",
    "                            color=Color,\n",
    "                            fill_color=Color).add_to(Map)\n",
    "    \n",
    "china_result_20.apply(lambda x: draw(china_Map,[x['lat'],x['lon']],x['tone'],x['country']),axis = 1)\n",
    "us_result_20.apply(lambda x: draw(us_Map,[x['lat'],x['lon']],x['tone'],x['country']),axis = 1)\n",
    "uk_result_20.apply(lambda x: draw(uk_Map,[x['lat'],x['lon']],x['tone'],x['country']),axis = 1)\n",
    "\n",
    "        \n",
    "\n",
    "china_Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "us_Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uk_Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "china_Map.save(theme+'_china.html')\n",
    "us_Map.save(theme+'_us.html')\n",
    "uk_Map.save(theme+'_uk.html')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
